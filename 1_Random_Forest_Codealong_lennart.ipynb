{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please copy each line into a different cell in Jupyter notebook and a) comment on what every line is doing to demonstrate your understanding b) run/print as many lines of code in each cell as possible to see the output. This should help your understanding of Random Forest implementation. Lets keep moving forward!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, with libraries such as Scikit-Learn, it’s now easy to implement hundreds of machine learning algorithms in Python. \n",
    "\n",
    "\n",
    "\n",
    "It’s so easy that we often don’t need any underlying knowledge of how the model works in order to use it. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "While knowing all the details is not necessary, it’s still helpful to have an idea of how a machine learning model works under the hood. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "This lets us diagnose the model when it’s underperforming or explain how it makes decisions, which is crucial if we want to convince others to trust our models.\n",
    "\n",
    "\n",
    "\n",
    "In this Codealong notebook, we’ll look at how to build and use the Random Forest in Python. In addition to seeing the code, we’ll try to get an understanding of how this model works.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   12.7s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of nodes 13396\n",
      "Average maximum depth 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Baseline: 1.0 Test: 0.96 Train: 1.0\n",
      "Precision Baseline: 0.81 Test: 0.88 Train: 1.0\n",
      "Roc Baseline: 0.5 Test: 0.87 Train: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[ 2438  3136]\n",
      " [ 1043 23299]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RSEED = 50\n",
    "\n",
    "# Load in data\n",
    "df = pd.read_csv('https://s3.amazonaws.com/projects-rf/clean_data.csv')\n",
    "\n",
    "# Full dataset: https://www.kaggle.com/cdc/behavioral-risk-factor-surveillance-system\n",
    "\n",
    "# Extract the labels\n",
    "labels = np.array(df.pop('label'))\n",
    "\n",
    "# 30% examples in test data\n",
    "train, test, train_labels, test_labels = train_test_split(df,\n",
    "                                         labels, \n",
    "                                         stratify = labels,\n",
    "                                         test_size = 0.3, \n",
    "                                         random_state = RSEED)\n",
    "\n",
    "# Imputation of missing values\n",
    "train = train.fillna(train.mean())\n",
    "test = test.fillna(test.mean())\n",
    "\n",
    "# Features for feature importances\n",
    "features = list(train.columns)\n",
    "\n",
    "# Create the model with 100 trees\n",
    "model = RandomForestClassifier(n_estimators=100, \n",
    "                               random_state=RSEED, \n",
    "                               max_features = 'sqrt',\n",
    "                               n_jobs=-1, verbose = 1)\n",
    "\n",
    "# Fit on training data\n",
    "model.fit(train, train_labels)\n",
    "\n",
    "\n",
    "n_nodes = []\n",
    "max_depths = []\n",
    "\n",
    "# Stats about the trees in random forest\n",
    "for ind_tree in model.estimators_:\n",
    "    n_nodes.append(ind_tree.tree_.node_count)\n",
    "    max_depths.append(ind_tree.tree_.max_depth)\n",
    "    \n",
    "print(f'Average number of nodes {int(np.mean(n_nodes))}')\n",
    "print(f'Average maximum depth {int(np.mean(max_depths))}')\n",
    "\n",
    "# Training predictions (to demonstrate overfitting)\n",
    "train_rf_predictions = model.predict(train)\n",
    "train_rf_probs = model.predict_proba(train)[:, 1]\n",
    "\n",
    "# Testing predictions (to determine performance)\n",
    "rf_predictions = model.predict(test)\n",
    "rf_probs = model.predict_proba(test)[:, 1]\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot formatting\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams['font.size'] = 18\n",
    "\n",
    "def evaluate_model(predictions, probs, train_predictions, train_probs):\n",
    "    \"\"\"Compare machine learning model to baseline performance.\n",
    "    Computes statistics and shows ROC curve.\"\"\"\n",
    "    \n",
    "    baseline = {}\n",
    "    \n",
    "    baseline['recall'] = recall_score(test_labels, \n",
    "                                     [1 for _ in range(len(test_labels))])\n",
    "    baseline['precision'] = precision_score(test_labels, \n",
    "                                      [1 for _ in range(len(test_labels))])\n",
    "    baseline['roc'] = 0.5\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    results['recall'] = recall_score(test_labels, predictions)\n",
    "    results['precision'] = precision_score(test_labels, predictions)\n",
    "    results['roc'] = roc_auc_score(test_labels, probs)\n",
    "    \n",
    "    train_results = {}\n",
    "    train_results['recall'] = recall_score(train_labels, train_predictions)\n",
    "    train_results['precision'] = precision_score(train_labels, train_predictions)\n",
    "    train_results['roc'] = roc_auc_score(train_labels, train_probs)\n",
    "    \n",
    "    for metric in ['recall', 'precision', 'roc']:\n",
    "        print(f'{metric.capitalize()} Baseline: {round(baseline[metric], 2)} Test: {round(results[metric], 2)} Train: {round(train_results[metric], 2)}')\n",
    "    \n",
    "    # Calculate false positive rates and true positive rates\n",
    "    base_fpr, base_tpr, _ = roc_curve(test_labels, [1 for _ in range(len(test_labels))])\n",
    "    model_fpr, model_tpr, _ = roc_curve(test_labels, probs)\n",
    "\n",
    "    plt.figure(figsize = (8, 6))\n",
    "    plt.rcParams['font.size'] = 16\n",
    "    \n",
    "    # Plot both curves\n",
    "    plt.plot(base_fpr, base_tpr, 'b', label = 'baseline')\n",
    "    plt.plot(model_fpr, model_tpr, 'r', label = 'model')\n",
    "    plt.legend();\n",
    "    plt.xlabel('False Positive Rate'); \n",
    "    plt.ylabel('True Positive Rate'); plt.title('ROC Curves');\n",
    "    plt.show();\n",
    "\n",
    "evaluate_model(rf_predictions, rf_probs, train_rf_predictions, train_rf_probs)\n",
    "plt.savefig('roc_auc_curve.png')\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Oranges):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    Source: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize = (10, 10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, size = 24)\n",
    "    plt.colorbar(aspect=4)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, size = 14)\n",
    "    plt.yticks(tick_marks, classes, size = 14)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    \n",
    "    # Labeling the plot\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), fontsize = 20,\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        \n",
    "    plt.grid(None)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', size = 18)\n",
    "    plt.xlabel('Predicted label', size = 18)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(test_labels, rf_predictions)\n",
    "plot_confusion_matrix(cm, classes = ['Poor Health', 'Good Health'],\n",
    "                      title = 'Health Confusion Matrix')\n",
    "\n",
    "plt.savefig('cm.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
